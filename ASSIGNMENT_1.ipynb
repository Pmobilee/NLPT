{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Linguistic analysis using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization (1 point)\n",
    "Process the dataset using the spaCy package and extract the following information:\n",
    "**Number of tokens:\n",
    "Number of types:  \n",
    "Number of words:\n",
    "Average number of words per sentence:\n",
    "Average word length: \n",
    "Provide the definition that you used to determine words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads a small English model trained on web data.\n",
    "# For other models and languages check: https://spacy.io/models\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "with open(f\"{cwd}/data/preprocessed/train/sentences.txt\", \"r\") as file:\n",
    "    train_dataset = file.read()\n",
    "doc = nlp(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our test doc for later testing\n",
    "test_input = \"I have an awesome cat. It's sitting on the mat that I bought yesterday.\"\n",
    "doc_test = nlp(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 16130\n",
      "Num words: 13895\n",
      "Num types: 3722\n",
      "Words per sentence: 19.352\n",
      "Average length per word: 4.432\n"
     ]
    }
   ],
   "source": [
    "# Counting words and frequencies\n",
    "word_frequencies = Counter()\n",
    "words_per_sencence = []\n",
    "words_length = []\n",
    "avg_num_words = []\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    lengths = []\n",
    "    num_words = 0\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            lengths.append(len(token.text))\n",
    "            num_words += 1\n",
    "    avg_num_words.append(num_words)\n",
    "    words_length.append(np.mean(lengths))\n",
    "    word_frequencies.update(words)\n",
    "    \n",
    "# print(word_frequencies)\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "avg_num_words_sentence = np.mean(avg_num_words)\n",
    "avg_num_word_length = np.nanmean(words_length)\n",
    "\n",
    "\n",
    "print(f\"Num tokens: {num_tokens}\\nNum words: {num_words}\\nNum types: {num_types}\\nWords per sentence: {np.around(avg_num_words_sentence, 3)}\\nAverage length per word: {np.around(avg_num_word_length, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: I saw that the words also contained \"/n\", so we might want to remove that one? In that case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 16130\n",
      "Num words: 13242\n",
      "Num types: 3721\n",
      "Words per sentence: 18.443\n",
      "Average length per word: 4.891\n"
     ]
    }
   ],
   "source": [
    "# Counting words and frequencies\n",
    "word_frequencies = Counter()\n",
    "words_per_sencence = []\n",
    "words_length = []\n",
    "avg_num_words = []\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    lengths = []\n",
    "    num_words = 0\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not (token.is_punct or token.text == \"\\n\"):\n",
    "            words.append(token.text)\n",
    "            lengths.append(len(token.text))\n",
    "            num_words += 1\n",
    "    avg_num_words.append(num_words)\n",
    "    words_length.append(np.mean(lengths))\n",
    "    word_frequencies.update(words)\n",
    "\n",
    "# print(word_frequencies)\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "avg_num_words_sentence = np.mean(avg_num_words)\n",
    "avg_num_word_length = np.nanmean(words_length)\n",
    "\n",
    "\n",
    "print(f\"Num tokens: {num_tokens}\\nNum words: {num_words}\\nNum types: {num_types}\\nWords per sentence: {np.around(avg_num_words_sentence, 3)}\\nAverage length per word: {np.around(avg_num_word_length, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our definition of a word: Any token not part of the punctuation set or the \"\\n\" character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Classes (1.5 points)\n",
    "\n",
    "Run the default part-of-speech tagger on the dataset and identify the ten most frequent \n",
    "POS tags. Complete the table below for these ten tags (the tagger in the model \n",
    "en_core_web_sm is trained on the PENN Treebank tagset). "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "787fd427633f043040e87f5eaa02492cbef96ac8667c95e7d05ab643b7636ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLPT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
