{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Linguistic analysis using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization (1 point)\n",
    "Process the dataset using the spaCy package and extract the following information:\n",
    "**Number of tokens:\n",
    "Number of types:  \n",
    "Number of words:\n",
    "Average number of words per sentence:\n",
    "Average word length: \n",
    "Provide the definition that you used to determine words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (61.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads a small English model trained on web data.\n",
    "# For other models and languages check: https://spacy.io/models\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "with open(f\"{cwd}/data/preprocessed/train/sentences.txt\", \"r\") as file:\n",
    "    train_dataset = file.read()\n",
    "doc = nlp(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our test doc for later testing\n",
    "test_input = \"I have an awesome cat. It's sitting on the mat that I bought yesterday.\"\n",
    "doc_test = nlp(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 16130\n",
      "Num words: 13895\n",
      "Num types: 3722\n",
      "Words per sentence: 19.352\n",
      "Average length per word: 4.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/damion/anaconda3/envs/NLPT/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Counting words and frequencies\n",
    "word_frequencies = Counter()\n",
    "words_per_sencence = []\n",
    "words_length = []\n",
    "avg_num_words = []\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    lengths = []\n",
    "    num_words = 0\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            lengths.append(len(token.text))\n",
    "            num_words += 1\n",
    "    avg_num_words.append(num_words)\n",
    "    words_length.append(np.mean(lengths))\n",
    "    word_frequencies.update(words)\n",
    "    \n",
    "# print(word_frequencies)\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "avg_num_words_sentence = np.mean(avg_num_words)\n",
    "avg_num_word_length = np.nanmean(words_length)\n",
    "\n",
    "\n",
    "print(f\"Num tokens: {num_tokens}\\nNum words: {num_words}\\nNum types: {num_types}\\nWords per sentence: {np.around(avg_num_words_sentence, 3)}\\nAverage length per word: {np.around(avg_num_word_length, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: I saw that the words also contained \"/n\", so we might want to remove that one? In that case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 16130\n",
      "Num words: 13242\n",
      "Num types: 3721\n",
      "Words per sentence: 18.443\n",
      "Average length per word: 4.891\n"
     ]
    }
   ],
   "source": [
    "# Counting words and frequencies\n",
    "word_frequencies = Counter()\n",
    "words_per_sencence = []\n",
    "words_length = []\n",
    "avg_num_words = []\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    lengths = []\n",
    "    num_words = 0\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not (token.is_punct or token.text == \"\\n\"):\n",
    "            words.append(token.text)\n",
    "            lengths.append(len(token.text))\n",
    "            num_words += 1\n",
    "    avg_num_words.append(num_words)\n",
    "    words_length.append(np.mean(lengths))\n",
    "    word_frequencies.update(words)\n",
    "\n",
    "# print(word_frequencies)\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "avg_num_words_sentence = np.mean(avg_num_words)\n",
    "avg_num_word_length = np.nanmean(words_length)\n",
    "\n",
    "\n",
    "print(f\"Num tokens: {num_tokens}\\nNum words: {num_words}\\nNum types: {num_types}\\nWords per sentence: {np.around(avg_num_words_sentence, 3)}\\nAverage length per word: {np.around(avg_num_word_length, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our definition of a word: Any token not part of the punctuation set or the \"\\n\" character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Classes (1.5 points)\n",
    "\n",
    "Run the default part-of-speech tagger on the dataset and identify the ten most frequent \n",
    "POS tags. Complete the table below for these ten tags (the tagger in the model \n",
    "en_core_web_sm is trained on the PENN Treebank tagset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Finegrained POS-tag</th>\n",
       "      <th>Universal POS-tag</th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Relative Tag Frequency (%)</th>\n",
       "      <th>3 most frequent tokens</th>\n",
       "      <th>Example of infrequent token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>2066</td>\n",
       "      <td>0.13</td>\n",
       "      <td>[month, baby, agar]</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2060</td>\n",
       "      <td>0.13</td>\n",
       "      <td>[ROS, Police, Virginia]</td>\n",
       "      <td>Navy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[alongside, of, with]</td>\n",
       "      <td>By</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>1313</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[an, the, a]</td>\n",
       "      <td>Each</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>868</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[old, different, regional]</td>\n",
       "      <td>Sebastian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>774</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[children, years, concentrations]</td>\n",
       "      <td>areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>699</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[,, ;, …]</td>\n",
       "      <td>…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>655</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[., ?, !]</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPACE</td>\n",
       "      <td>_SP</td>\n",
       "      <td>653</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[\\n]</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>454</td>\n",
       "      <td>0.03</td>\n",
       "      <td>[thought, aged, represented]</td>\n",
       "      <td>acquitted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Finegrained POS-tag Universal POS-tag  Occurrences  \\\n",
       "0                NOUN                NN         2066   \n",
       "1               PROPN               NNP         2060   \n",
       "2                 ADP                IN         1600   \n",
       "3                 DET                DT         1313   \n",
       "4                 ADJ                JJ          868   \n",
       "5                NOUN               NNS          774   \n",
       "6               PUNCT                 ,          699   \n",
       "7               PUNCT                 .          655   \n",
       "8               SPACE               _SP          653   \n",
       "9                VERB               VBN          454   \n",
       "\n",
       "   Relative Tag Frequency (%)             3 most frequent tokens  \\\n",
       "0                        0.13                [month, baby, agar]   \n",
       "1                        0.13            [ROS, Police, Virginia]   \n",
       "2                        0.10              [alongside, of, with]   \n",
       "3                        0.08                       [an, the, a]   \n",
       "4                        0.05         [old, different, regional]   \n",
       "5                        0.05  [children, years, concentrations]   \n",
       "6                        0.04                          [,, ;, …]   \n",
       "7                        0.04                          [., ?, !]   \n",
       "8                        0.04                               [\\n]   \n",
       "9                        0.03       [thought, aged, represented]   \n",
       "\n",
       "  Example of infrequent token  \n",
       "0                     project  \n",
       "1                        Navy  \n",
       "2                          By  \n",
       "3                        Each  \n",
       "4                   Sebastian  \n",
       "5                       areas  \n",
       "6                           …  \n",
       "7                           !  \n",
       "8                          \\n  \n",
       "9                   acquitted  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = []\n",
    "\n",
    "NN_Noun = []\n",
    "NNP_Propn = []\n",
    "IN_Adp = []\n",
    "DT_Det = []\n",
    "JJ_Adj = []\n",
    "NNS_Noun = []\n",
    "COMMA_Punct = []\n",
    "PERIOD_Punct = []\n",
    "SP_Space = []\n",
    "VBN_Verb = []\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.pos_, token.tag_)\n",
    "    pos_list.append('{}, {}'.format(token.pos_, token.tag_))\n",
    "    if ('{}, {}'.format(token.pos_, token.tag_) == 'NOUN, NN'):\n",
    "        NN_Noun.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'PROPN, NNP'):\n",
    "        NNP_Propn.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'ADP, IN'):\n",
    "        IN_Adp.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'DET, DT'):\n",
    "        DT_Det.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'ADJ, JJ'):\n",
    "        JJ_Adj.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'NOUN, NNS'):\n",
    "        NNS_Noun.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'PUNCT, ,'):\n",
    "        COMMA_Punct.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'PUNCT, .'):\n",
    "        PERIOD_Punct.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'SPACE, _SP'):\n",
    "        SP_Space.append(token.text)\n",
    "\n",
    "    elif ('{}, {}'.format(token.pos_, token.tag_) == 'VERB, VBN'):\n",
    "        VBN_Verb.append(token.text)\n",
    "\n",
    "\n",
    "pos_frequencies = Counter(pos_list)\n",
    "rtf = []\n",
    "rtf.append(round(pos_frequencies['NOUN, NN']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['PROPN, NNP']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['ADP, IN']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['DET, DT']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['ADJ, JJ']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['NOUN, NNS']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['PUNCT, ,']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['PUNCT, .']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['SPACE, _SP']/sum(pos_frequencies.values()), 2))\n",
    "rtf.append(round(pos_frequencies['VERB, VBN']/sum(pos_frequencies.values()), 2))\n",
    "\n",
    "finegrained = [\"NOUN\", \"PROPN\", \"ADP\", \"DET\", \"ADJ\", \"NOUN\", \"PUNCT\", \"PUNCT\", \"SPACE\", \"VERB\"]\n",
    "universal = [\"NN\", \"NNP\", \"IN\", \"DT\", \"JJ\", \"NNS\", \",\", \".\", \"_SP\", \"VBN\"]\n",
    "occurrences = [len(NN_Noun), len(NNP_Propn), len(IN_Adp), len(DT_Det), len(JJ_Adj), len(NNS_Noun), len(COMMA_Punct), len(PERIOD_Punct), len(SP_Space), len(VBN_Verb)]\n",
    "\n",
    "most_frequent = []\n",
    "most_frequent.append(list(Counter(NN_Noun))[0:3])\n",
    "most_frequent.append(list(Counter(NNP_Propn))[0:3])\n",
    "most_frequent.append(list(Counter(IN_Adp))[0:3])\n",
    "most_frequent.append(list(Counter(DT_Det))[0:3])\n",
    "most_frequent.append(list(Counter(JJ_Adj))[0:3])\n",
    "most_frequent.append(list(Counter(NNS_Noun))[0:3])\n",
    "most_frequent.append(list(Counter(COMMA_Punct))[0:3])\n",
    "most_frequent.append(list(Counter(PERIOD_Punct))[0:3])\n",
    "most_frequent.append(list(Counter(SP_Space))[0:3])\n",
    "most_frequent.append(list(Counter(VBN_Verb))[0:3])\n",
    "\n",
    "least_frequent = []\n",
    "least_frequent.append(list(Counter(NN_Noun))[-1])\n",
    "least_frequent.append(list(Counter(NNP_Propn))[-1])\n",
    "least_frequent.append(list(Counter(IN_Adp))[-1])\n",
    "least_frequent.append(list(Counter(DT_Det))[-1])\n",
    "least_frequent.append(list(Counter(JJ_Adj))[-1])\n",
    "least_frequent.append(list(Counter(NNS_Noun))[-1])\n",
    "least_frequent.append(list(Counter(COMMA_Punct))[-1])\n",
    "least_frequent.append(list(Counter(PERIOD_Punct))[-1])\n",
    "least_frequent.append(list(Counter(SP_Space))[-1])\n",
    "least_frequent.append(list(Counter(VBN_Verb))[-1])\n",
    "\n",
    "word_class_table = pd.DataFrame({\"Finegrained POS-tag\":finegrained, \"Universal POS-tag\":universal, \"Occurrences\": occurrences, \"Relative Tag Frequency (%)\" : rtf, \"3 most frequent tokens\" : most_frequent, \"Example of infrequent token\": least_frequent})\n",
    "word_class_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 N-Grams (1.5 points)\n",
    "Calculate the distribution of n-grams and provide the 3 most frequent\n",
    "Token bigrams,\n",
    "Token trigrams,\n",
    "POS bigrams,\n",
    "and POS trigrams"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "787fd427633f043040e87f5eaa02492cbef96ac8667c95e7d05ab643b7636ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLPT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
